{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937b58ce-053f-4986-9bb0-b23610966f4d",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Group Project by Dolphin Sharma and Charles Crocicchia\n",
    "\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4735c9d3-fd41-4c49-95cc-3527a346a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a223cdd-24b7-465e-a834-9269e12c8253",
   "metadata": {},
   "source": [
    "## Load & Merge Data\n",
    "\n",
    "Loaded the relevant datasets for soybean data, corn data, and economic data (e.g., employment, income, poverty levels). These datasets were merged based on common keys like state and year to create a comprehensive dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55950463-81a1-4601-a1dc-7cfa06b9982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and merge all data files within a directory\n",
    "def load_all_data_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Loads all CSV files from a specified directory and returns a list of dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    directory_path (str): The path to the directory containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of pandas DataFrames, each representing the data from one CSV file.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            data = pd.read_csv(file_path)\n",
    "            all_data.append(data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e5e09a-e398-4205-b7f6-c9d56e676016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge corn data\n",
    "def merge_corn_data(corn_dir):\n",
    "    \"\"\"\n",
    "    Merges all corn data CSV files from the specified directory into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    corn_dir (str): The path to the directory containing corn-related CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame resulting from merging the corn data files.\n",
    "    \"\"\"\n",
    "    corn_data = load_all_data_from_directory(corn_dir)\n",
    "    corn_merged = pd.concat(corn_data, axis=1, join='inner')\n",
    "    return corn_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d43f7c-f105-4e9c-ba0d-0beedacaa966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge soybean data\n",
    "def merge_soybean_data(soybean_dir):\n",
    "    \"\"\"\n",
    "    Merges all soybean data CSV files from the specified directory into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    soybean_dir (str): The path to the directory containing soybean-related CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame resulting from merging the soybean data files.\n",
    "    \"\"\"\n",
    "    soybean_data = load_all_data_from_directory(soybean_dir)\n",
    "    soybean_merged = pd.concat(soybean_data, axis=1, join='inner')\n",
    "    return soybean_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef30ff6-7116-40be-a042-a23375b7de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge economic data\n",
    "def merge_economic_data(economic_dir):\n",
    "    \"\"\"\n",
    "    Merges all economic data CSV files from the specified directory into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    economic_dir (str): The path to the directory containing economic-related CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame resulting from merging the economic data files.\n",
    "    \"\"\"\n",
    "    economic_data = load_all_data_from_directory(economic_dir)\n",
    "    economic_merged = pd.concat(economic_data, axis=1, join='inner')\n",
    "    return economic_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98b81ba-4098-4006-83f6-a7c16e2c7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for the data\n",
    "corn_dir = r'../data/clean_data/corn_data/'\n",
    "soybean_dir = r'../data/clean_data/soybean_data/'\n",
    "economic_dir = r'../data/clean_data/economic_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d39342f-7678-49d1-9997-d62b83179cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data\n",
    "corn_merged = merge_corn_data(corn_dir)\n",
    "soybean_merged = merge_soybean_data(soybean_dir)\n",
    "economic_merged = merge_economic_data(economic_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f143f839-1e90-4e21-a275-63b91d5ef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge corn and soybean data with economic data\n",
    "corn_soybean_merged = pd.concat([corn_merged, soybean_merged], axis=1, join='inner')\n",
    "final_merged_data = pd.concat([corn_soybean_merged, economic_merged], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9dcefd-51f5-4d39-93b2-c4030a245ec8",
   "metadata": {},
   "source": [
    "## Quick Data Cleanup After Merge\n",
    "\n",
    "Removed unnecessary columns 'Unnamed', removed duplicated column(s), and removed numerical suffices from column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f655deeb-792b-4301-9cda-2234be50b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data by removing unnecessary columns\n",
    "final_merged_data_clean = final_merged_data.loc[:, ~final_merged_data.columns.str.contains('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b18f709-d7ba-4a4e-8916-8fffe992110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns (like 'State.1')\n",
    "final_merged_data_clean = final_merged_data_clean.loc[:, ~final_merged_data_clean.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e7613a7-6725-422e-baa6-59103641fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numerical suffixes from column names\n",
    "final_merged_data_clean.columns = final_merged_data_clean.columns.str.replace(r'\\.\\d+$', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c6eb4-6e66-44e3-a3f8-121692f12a9d",
   "metadata": {},
   "source": [
    "## Renaming/Categorizing Columns \n",
    "\n",
    "Columns are renamed or categorized based on the type of data they represented (e.g., corn, soybean, or economic indicators) to make them easier to work with and identify during the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453c5dac-1466-4e13-a718-e2a99211ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming and categorizing columns\n",
    "final_merged_data_clean.rename(columns={\n",
    "    'Value': 'Corn_Acres_Planted',\n",
    "    '2008': 'Corn_Cash_Receipts',\n",
    "    '2009': 'Corn_Production_Bushels',\n",
    "    '2010': 'Corn_Production_Dollars',\n",
    "    '2011': 'Corn_Yield_Bushels',\n",
    "    '2012': 'Soybean_Acres_Planted',\n",
    "    '2013': 'Soybean_Cash_Receipts',\n",
    "    '2014': 'Soybean_Production_Bushels',\n",
    "    '2015': 'Soybean_Production_Dollars',\n",
    "    '2016': 'Soybean_Yield_Bushels',\n",
    "    '2017': 'Employment_Rural',\n",
    "    '2018': 'Personal_Income_Rural',\n",
    "    '2019': 'Poverty_Levels',\n",
    "    '2020': 'Economic_Indicator_2020',\n",
    "    '2021': 'Economic_Indicator_2021',\n",
    "    '2022': 'Economic_Indicator_2022',\n",
    "    '2023': 'Economic_Indicator_2023',\n",
    "    '2000': 'Historical_Economic_2000',\n",
    "    '2001': 'Historical_Economic_2001',\n",
    "    '2002': 'Historical_Economic_2002',\n",
    "    '2003': 'Historical_Economic_2003',\n",
    "    '2004': 'Historical_Economic_2004',\n",
    "    '2005': 'Historical_Economic_2005',\n",
    "    '2006': 'Historical_Economic_2006',\n",
    "    '2007': 'Historical_Economic_2007'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda46db-b918-4760-ad68-c80875625b99",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Missing values in the dataset are addressed by dropping rows or columns with too many missing entries. Alternatively, imputation strategies (filling with median) are used for columns with fewer missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e623ac97-061c-482e-94cc-75e1ec8e6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle missing values\n",
    "def impute_missing_values(data, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Imputes missing values in the specified columns using the mean of the columns.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The input DataFrame containing data with missing values.\n",
    "    columns (list): A list of column names for which missing values need to be imputed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913b8fe-3605-4660-ae1c-17c735f45a96",
   "metadata": {},
   "source": [
    "## Dealing with Collinearity\n",
    "\n",
    "The Variance Inflation Factor (VIF) is calculated to check for multicollinearity among the features. Features with high VIF values are either dropped or combined to reduce collinearity, which could negatively impact the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17722c64-ad08-4906-b94c-be6beb2913ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate VIF (Variance Inflation Factor)\n",
    "def calculate_vif(data, features):\n",
    "    \"\"\"\n",
    "    Calculates the Variance Inflation Factor (VIF) for a given set of features in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The input DataFrame containing the feature data.\n",
    "    features (list): A list of feature column names for which VIF will be calculated.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing each feature and its corresponding VIF score.\n",
    "    \"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = features\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(data[features].values, i) for i in range(len(features))]\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e79ad1-5a9f-499e-9312-951f903185d7",
   "metadata": {},
   "source": [
    "## Functions for Preprocessing Data by Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9345aab-8665-4f5f-a189-5e493d2f82df",
   "metadata": {},
   "source": [
    "### Scaling in Linear Regression\n",
    "\n",
    "Certain columns with large values (e.g., corn cash receipts, soybean production) are scaled to ensure that all features were on a similar scale. This step is important for models sensitive to feature scaling (e.g., linear regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d243e3ef-fd61-41b2-9da9-eb095cc2b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for Linear Regression (with scaling for all numerica columns and VIF calculation)\n",
    "def preprocess_for_linear_regression(data):\n",
    "    \"\"\"\n",
    "    Preprocesses data for linear regression, including imputing missing values and scaling the features.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The input DataFrame to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A preprocessed DataFrame suitable for linear regression.\n",
    "    \"\"\"\n",
    "    # Imputing missing values\n",
    "    columns_to_impute = [\n",
    "        'Corn_Acres_Planted', 'Soybean_Acres_Planted', 'Employment_Rural',\n",
    "        'Corn_Cash_Receipts', 'Corn_Production_Bushels', 'Corn_Production_Dollars', 'Corn_Yield_Bushels',\n",
    "        'Soybean_Cash_Receipts', 'Soybean_Production_Bushels', 'Soybean_Production_Dollars', 'Soybean_Yield_Bushels'\n",
    "    ]\n",
    "    data = impute_missing_values(data, columns_to_impute)\n",
    "    \n",
    "    # Scaling the data (including all numeric columns that should be scaled)\n",
    "    scaler = StandardScaler()\n",
    "    data[columns_to_impute] = scaler.fit_transform(data[columns_to_impute])\n",
    "    \n",
    "    # Calculate VIF for linear regression (only including primary features to avoid overfitting)\n",
    "    features = ['Corn_Acres_Planted', 'Soybean_Acres_Planted', 'Employment_Rural']\n",
    "    vif_data = calculate_vif(data, features)\n",
    "    print(\"Variance Inflation Factors:\\n\", vif_data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fe1183e-ddfd-4c24-9886-41b6508fc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for Decision Tree (no scaling needed)\n",
    "def preprocess_for_decision_tree(data):\n",
    "        \"\"\"\n",
    "    Preprocesses data for decision tree models, focusing on imputing missing values.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The input DataFrame to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A preprocessed DataFrame suitable for decision tree models.\n",
    "    \"\"\"\n",
    "    return impute_missing_values(data, ['Corn_Acres_Planted', 'Soybean_Acres_Planted', 'Employment_Rural'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7a2636d-460e-4997-b506-cc2289e1a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for Random Forest (no scaling needed)\n",
    "def preprocess_for_random_forest(data):\n",
    "    \"\"\"\n",
    "    Preprocesses data for random forest models, focusing on imputing missing values.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): The input DataFrame to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A preprocessed DataFrame suitable for random forest models.\n",
    "    \"\"\"\n",
    "    return impute_missing_values(data, ['Corn_Acres_Planted', 'Soybean_Acres_Planted', 'Employment_Rural'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b82a7441-0cb0-4974-9988-61d15fa9f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation Factors:\n",
      "                  Feature         VIF\n",
      "0     Corn_Acres_Planted    1.237742\n",
      "1  Soybean_Acres_Planted  158.022893\n",
      "2       Employment_Rural  160.861759\n"
     ]
    }
   ],
   "source": [
    "# Applying preprocessing to the cleaned dataset\n",
    "linear_regression_data = preprocess_for_linear_regression(final_merged_data_clean.copy())\n",
    "decision_tree_data = preprocess_for_decision_tree(final_merged_data_clean.copy())\n",
    "random_forest_data = preprocess_for_random_forest(final_merged_data_clean.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6dd7c96-a013-4b51-ba4e-ec498a9989ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the preprocessed data for each model\n",
    "linear_regression_data.to_csv(r'../data/processed_data/linear_regression_data.csv', index=False)\n",
    "decision_tree_data.to_csv(r'../data/processed_data/decision_tree_data.csv', index=False)\n",
    "random_forest_data.to_csv(r'../data/processed_data/random_forest_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
